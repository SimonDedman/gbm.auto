minbfbin
nrow(samples)
paste0("  binary bag fraction must be at least ", round(minbfbin, 3)), ". n = ", nrow(samples)
round(minbfbin, 3)
print(
paste0("  binary bag fraction must be at least ",
round(minbfbin, 3),
". n = ",
nrow(samples)
)
# if user has asked code to check for ZI, check it & set new ZI status
if (ZI == "CHECK") if (sum(samples[,resvar] == 0, na.rm = TRUE) / length(samples[,resvar]) >= 0.5) ZI = TRUE else ZI = FALSE
logem <- log1p(samples[,resvar])
dont  <- samples[,resvar]
# resvar should be LCS not 2
gbm.bfcheck(samples = Species_Counts, resvar = "LCS", ZI = "CHECK")
resvar = "LCS"
logem <- log1p(samples[,resvar])
dont  <- samples[,resvar]
if (ZI) {samples$grv <- logem} else {samples$grv <- dont}
grv_yes <- subset(samples, grv > 0) # nonzero subset for gaussian BRTs
# Minimum bag fraction for Gaussian
minbfgaus <- 21/nrow(grv_yes)
print(paste0("Gaussian bag fraction must be at least ", round(minbfgaus,3)), ". n = ", nrow(grv_yes))
print(
paste0("Gaussian bag fraction must be at least ",
round(minbfgaus,3),
". n = ",
nrow(grv_yes)
)
seq(0.001, 0.005, by = 0.0001)
seq(0.3, 0.9, by = 0.1)
seq(0.0001, 0.004, by = 0.0001)
seq(0.001, 0.005, by = 0.0001)
lr = seq(0.001, 0.005, by = 0.0001)
bf = seq(0.3, 0.9, by = 0.1)
41*7
6:13
list(0.0027, 0.0037)
View(e)
e@TPR
e@TNR
e@TPR + e@TNR
which.max(e@TPR + e@TNR)
e@t
e@t[which.max(e@TPR + e@TNR)]
Sensitivity + Specificity - 1
max(e@TPR + e@TNR - 1)
range(e@TPR + e@TNR - 1)
range(e@TPR)
range(e@TNR)
max(e@TPR + e@TNR - 1)
max(e@TPR + e@TNR)
e@TPR + e@FNR
Sensitivity
e@TPR[which.max(e@TPR + e@TNR)]
e@TPR / (e@TPR + e@FNR)
e@TNR[which.max(e@TPR + e@TNR)]
e@TPR[which.max(e@TPR + e@TNR)]
e@TPR + e@TNR
(e@TPR + e@TNR)/nrow(samples)
(e@TPR[which.max(e@TPR + e@TNR)] + e@TNR[which.max(e@TPR + e@TNR)])/nrow(samples)
e@TPR[which.max(e@TPR + e@TNR)]
e@TNR[which.max(e@TPR + e@TNR)]
(e@TPR[which.max(e@TPR + e@TNR)] + e@TNR[which.max(e@TPR + e@TNR)])
OverallAccuracy <- (e@TPR[which.max(e@TPR + e@TNR)] + e@TNR[which.max(e@TPR + e@TNR)])/nrow(samples)
e@FNR[which.max(e@TPR + e@TNR)]
e@FPR[which.max(e@TPR + e@TNR)] +
e@FPR[which.max(e@TPR + e@TNR)]
e@TPR[which.max(e@TPR + e@TNR)] +
e@TNR[which.max(e@TPR + e@TNR)]
e@FPR[which.max(e@TPR + e@TNR)] +
e@FNR[which.max(e@TPR + e@TNR)]
e@TPR[which.max(e@TPR + e@TNR)] +
e@TNR[which.max(e@TPR + e@TNR)] + # TP+TN+FP+FN
e@FPR[which.max(e@TPR + e@TNR)] +
e@FNR[which.max(e@TPR + e@TNR)]
e@TPR[which.max(e@TPR + e@TNR)] +
e@TNR[which.max(e@TPR + e@TNR)]
e@TPR[which.max(e@TPR + e@TNR)] + # TP+TN /
e@TNR[which.max(e@TPR + e@TNR)]
e@TPR[which.max(e@TPR + e@TNR)] + # TP+TN is just TSS + 1
e@TNR[which.max(e@TPR + e@TNR)] /
e@TPR[which.max(e@TPR + e@TNR)] +# TP+TN+FP+FN #Sums to 2, redundant
e@TNR[which.max(e@TPR + e@TNR)] +
e@FPR[which.max(e@TPR + e@TNR)] +
e@FNR[which.max(e@TPR + e@TNR)]
e@TPR[which.max(e@TPR + e@TNR)] +# TP+TN+FP+FN #Sums to 2, redundant
e@TNR[which.max(e@TPR + e@TNR)] +
e@FPR[which.max(e@TPR + e@TNR)] +
e@FNR[which.max(e@TPR + e@TNR)]
e@TPR[which.max(e@TPR + e@TNR)] + # TP+TN is just TSS + 1
e@TNR[which.max(e@TPR + e@TNR)]
e@TPR[which.max(e@TPR + e@TNR)] + # TP+TN is just TSS + 1
e@TNR[which.max(e@TPR + e@TNR)] /
e@TPR[which.max(e@TPR + e@TNR)] +# TP+TN+FP+FN #Sums to 2, redundant
e@TNR[which.max(e@TPR + e@TNR)] +
e@FPR[which.max(e@TPR + e@TNR)] +
e@FNR[which.max(e@TPR + e@TNR)]
(e@TPR[which.max(e@TPR + e@TNR)] + # TP+TN is just TSS + 1
e@TNR[which.max(e@TPR + e@TNR)]) /
(e@TPR[which.max(e@TPR + e@TNR)] +# TP+TN+FP+FN #Sums to 2, redundant
e@TNR[which.max(e@TPR + e@TNR)] +
e@FPR[which.max(e@TPR + e@TNR)] +
e@FNR[which.max(e@TPR + e@TNR)])
(e@TPR[which.max(e@TPR + e@TNR)] +# TP+TN+FP+FN #Sums to 2, redundant
e@TNR[which.max(e@TPR + e@TNR)] +
e@FPR[which.max(e@TPR + e@TNR)] +
e@FNR[which.max(e@TPR + e@TNR)])
(e@TPR[which.max(e@TPR + e@TNR)] + e@TNR[which.max(e@TPR + e@TNR)]) / 2
e@TPR[which.max(e@TPR + e@TNR)] + e@TNR[which.max(e@TPR + e@TNR)] / 2
# Accuracy: TP+TN / TP+TN+FP+FN true false positive negative.
Accuracy <- (e@TPR[which.max(e@TPR + e@TNR)] + e@TNR[which.max(e@TPR + e@TNR)]) / 2 # TP+TN is just TSS + 1, TP+TN+FP+FN #Sums to 2, redundant
(e@TPR[which.max(e@TPR + e@TNR)] + e@FPR[which.max(e@TPR + e@TNR)])
e@TPR[which.max(e@TPR + e@TNR)] / (e@TPR[which.max(e@TPR + e@TNR)] + e@FPR[which.max(e@TPR + e@TNR)])
# Precision: TP/TP+FP. Ignores true negatives. “X% of the predictions are right”
Precision <- e@TPR[which.max(e@TPR + e@TNR)] / (e@TPR[which.max(e@TPR + e@TNR)] + e@FPR[which.max(e@TPR + e@TNR)])
# Recall: TP/TP+FN: “Y% of actually existing things are captured”.
Recall <- e@TPR[which.max(e@TPR + e@TNR)] / (e@TPR[which.max(e@TPR + e@TNR)] + e@FNR[which.max(e@TPR + e@TNR)])
# Balance: precision vs recall curve. Workhorses.
# PxR/P+R = F score (P+R = harmonic mean).
# F1 score: P & R are equally rated. This is the most common one. F1 score importance depends on the project.
F1score <- (Precision * Recall) / (Precision + Recall)
# Balance: precision vs recall curve. Workhorses.
# PxR/P+R = F score (P+R = harmonic mean).
# F1 score: P & R are equally rated. This is the most common one. F1 score importance depends on the project.
F1score <- 2 * ((Precision * Recall) / (Precision + Recall))
BalancedAccuracy <- (Recall + Specificity) / 2
e@TPR[which.max(e@TPR + e@TNR)]
# -1 just makes the output range is 0:1 instead of 1:2 I think.
# If so this means Sensitivity is e@TPR[which.max(e@TPR + e@TNR)], which doesn't include
# (e@TPR + e@FNR) but it's a vector of 1s so is redundant. Same for Specificity
Sensitivity <- e@TPR[which.max(e@TPR + e@TNR)]
Specificity <- e@TNR[which.max(e@TPR + e@TNR)]
OverallAccuracy <- (e@TPR[which.max(e@TPR + e@TNR)] + e@TNR[which.max(e@TPR + e@TNR)])/nrow(samples)
# this seems like a weird metric since the numerator is 0:2 or 1:2 and the divisor could be tiny or huge
nSamples <- nrow(samples) # useful to include in the list
# Accuracy: TP+TN / TP+TN+FP+FN true false positive negative.
Accuracy <- (e@TPR[which.max(e@TPR + e@TNR)] + e@TNR[which.max(e@TPR + e@TNR)]) / 2 # TP+TN is just TSS + 1, TP+TN+FP+FN #Sums to 2, redundant
# Precision: TP/TP+FP. Ignores true negatives. “X% of the predictions are right”
Precision <- e@TPR[which.max(e@TPR + e@TNR)] / (e@TPR[which.max(e@TPR + e@TNR)] + e@FPR[which.max(e@TPR + e@TNR)])
# Recall: TP/TP+FN: “Y% of actually existing things are captured”.
Recall <- e@TPR[which.max(e@TPR + e@TNR)] / (e@TPR[which.max(e@TPR + e@TNR)] + e@FNR[which.max(e@TPR + e@TNR)])
# Balance: precision vs recall curve. Workhorses.
# PxR/P+R = F score (P+R = harmonic mean).
# F1 score: P & R are equally rated. This is the most common one. F1 score importance depends on the project.
F1score <- 2 * ((Precision * Recall) / (Precision + Recall))
BalancedAccuracy <- (Recall + Specificity) / 2
BalancedAccuracy
Precision + Recall
4 * Precision + Recall
4 * (Precision + Recall)
5 * ((Precision * Recall) / (4 * (Precision + Recall)))
5 * ((Precision * Recall) / (4 * Precision + Recall))
F2score <- 5 * ((Precision * Recall) / (4 * Precision + Recall))
e@np
e@na
e@auc
e@pauc
e@cor[[1]]
e@pcor
max(e@TPR + e@TNR-1)
View(e)
MLEval <- data.frame(Statistic = as.character(rep(NA, 30)),
Description = as.character(rep(NA, 30)),
Value = as.character(rep(NA, 30)))
MLEval[1,] <- c("Presence",
"n of presence data used",
e@np)
MLEval <- data.frame(Statistic = rep(NA, 30),
Description = rep(NA, 30),
Value = rep(NA, 30))
MLEval[1,] <- c("Presence",
"n of presence data used",
e@np)
MLEval[2,] <- c("Absence",
"n of absence data used",
e@na)
MLEval[3,] <- c("AUC",
"Area under the receiver operator (ROC) curve",
e@auc)
MLEval[4,] <- c("pAUC",
"p-value for the AUC (for the Wilcoxon test W statistic)",
e@pauc)
MLEval[5,] <- c("Cor",
"Correlation coefficient",
e@cor[[1]])
e@pauc
View(e)
length(e@pauc)
if (length(e@pauc) == 0) e@pauc <- NA
if (length(e@pauc) == 0) e@pauc <- 0
MLEval[4,] <- c("pAUC",
"p-value for the AUC (for the Wilcoxon test W statistic)",
e@pauc)
MLEval[5,] <- c("Cor",
"Correlation coefficient",
e@cor[[1]])
MLEval[6,] <- c("cor",
"p-value for correlation coefficient",
e@pcor)
# Steph Brodie's TSS which produces the same result as Allouche
# -1 just makes the output range is 0:1 instead of 1:2 I think.
# If so this means Sensitivity is e@TPR[which.max(e@TPR + e@TNR)], which doesn't include
# (e@TPR + e@FNR) but it's a vector of 1s so is redundant. Same for Specificity
MLEval[7,] <- c("TSS",
"True Skill Statistic",
max(e@TPR + e@TNR - 1))
# sensitivity: TP/(TP+FN)
MLEval[8,] <- c("Sensitivity",
"Sensitivity",
e@TPR[which.max(e@TPR + e@TNR)])
# specificity: TN/(FP+TN)
Specificity <- e@TNR[which.max(e@TPR + e@TNR)]
MLEval[9,] <- c("Specificity",
"Specificity",
Specificity)
# Accuracy: TP+TN / TP+TN+FP+FN true false positive negative.
# TP+TN is just TSS + 1, TP+TN+FP+FN #Sums to 2, redundant
MLEval[10,] <- c("Accuracy",
"Accuracy",
(e@TPR[which.max(e@TPR + e@TNR)] + e@TNR[which.max(e@TPR + e@TNR)]) / 2)
# Precision: TP/TP+FP. Ignores true negatives. “X% of the predictions are right”
Precision <- e@TPR[which.max(e@TPR + e@TNR)] / (e@TPR[which.max(e@TPR + e@TNR)] + e@FPR[which.max(e@TPR + e@TNR)])
MLEval[11,] <- c("Precision",
"X% of the predictions are right",
Precision)
# Recall: TP/TP+FN: “Y% of actually existing things are captured”.
Recall <- e@TPR[which.max(e@TPR + e@TNR)] / (e@TPR[which.max(e@TPR + e@TNR)] + e@FNR[which.max(e@TPR + e@TNR)])
MLEval[12,] <- c("Recall",
"Y% of actually existing things are captured",
Recall)
# https://www.corvil.com/kb/what-is-a-false-positive-rate
# Allouche et al 2006:
# overall accuracy: (TP+TN)/n
# this seems like a weird metric since the numerator is 0:2 or 1:2 and the divisor could be tiny or huge
MLEval[13,] <- c("OverallAccuracy",
"Overall Accuracy",
(e@TPR[which.max(e@TPR + e@TNR)] + e@TNR[which.max(e@TPR + e@TNR)])/nrow(samples))
# Balanced Accuracy, (Recall + Specificity) / 2
MLEval[14,] <- c("BalancedAccuracy",
"Balanced Accuracy",
(Recall + Specificity) / 2)
# Number of samples. Useful to include in the list
MLEval[15,] <- c("nSamples",
"Number of samples",
nrow(samples))
# Balance: precision vs recall curve. Workhorses.
# PxR/P+R = F score (P+R = harmonic mean).
# F1 score: P & R are equally rated. This is the most common one. F1 score importance depends on the project.
MLEval[16,] <- c("F1score",
"P & R equally rated, score importance depends on project",
2 * ((Precision * Recall) / (Precision + Recall)))
# F2 score: weighted average of Precision & Recall
MLEval[17,] <- c("F2score",
"weighted average of P & R",
5 * ((Precision * Recall) / (4 * Precision + Recall)))
# Threshold which produces the best combo of TPR & TNR
# t: vector of thresholds used to compute confusion matrices
MLEval[18,] <- c("Threshold",
"Threshold which produced best combo of TPR & TNR",
e@t[which.max(e@TPR + e@TNR)])
# e@prevalence: Prevalence
MLEval[19,] <- c("Prevalence",
"Prevalence",
e@prevalence[which.max(e@TPR + e@TNR)])
# e@ODP: Overall diagnostic power
MLEval[20,] <- c("ODP",
"Overall diagnostic power",
e@ODP[which.max(e@TPR + e@TNR)])
# e@CCR: Correct classification rate
MLEval[21,] <- c("CCR",
"Correct classification rate",
e@CCR[which.max(e@TPR + e@TNR)])
# e@TPR: True positive rate
MLEval[22,] <- c("TPR",
"True positive rate",
e@TPR[which.max(e@TPR + e@TNR)])
# e@TNR: True negative rate
MLEval[23,] <- c("TNR",
"True negative rate",
e@TNR[which.max(e@TPR + e@TNR)])
# e@FPR: False positive rate
MLEval[24,] <- c("FPR",
"False positive rate",
e@FPR[which.max(e@TPR + e@TNR)])
# e@FNR: False negative rate
MLEval[25,] <- c("FNR",
"False negative rate",
e@FNR[which.max(e@TPR + e@TNR)])
# e@PPP: Positive predictive power
MLEval[26,] <- c("PPP",
"Positive predictive power",
e@PPP[which.max(e@TPR + e@TNR)])
# e@NPP: Negative predictive power
MLEval[27,] <- c("NPP",
"Negative predictive power",
e@NPP[which.max(e@TPR + e@TNR)])
# e@MCR: Misclassification rate
MLEval[28,] <- c("MCR",
"Misclassification rate",
e@MCR[which.max(e@TPR + e@TNR)])
# e@OR: Odds-ratio
MLEval[29,] <- c("OR",
"Odds-ratio",
e@OR[which.max(e@TPR + e@TNR)])
# e@kappa: Cohen's kappa
MLEval[30,] <- c("kappa",
"Cohen's kappa",
e@kappa[which.max(e@TPR + e@TNR)])
View(MLEval)
plot(e, 'ROC')
plot(e, 'kappa')
plot(e, 'FPR')
plot(e, 'prevalence')
# Fielding, A. H. & J.F. Bell, 1997. A review of methods for the assessment of prediction errors in conservation presence/absence models. Environmental Conservation 24: 38-49
# Liu, C., M. White & G. Newell, 2011. Measuring and comparing the accuracy of species distribution models with presence-absence data. Ecography 34: 232-243.
MLEvalLength <- 31
# Improve descriptions####
MLEval <- data.frame(Statistic = rep(NA, MLEvalLength),
Description = rep(NA, MLEvalLength),
Value = rep(NA, MLEvalLength))
# Fielding, A. H. & J.F. Bell, 1997. A review of methods for the assessment of prediction errors in conservation presence/absence models. Environmental Conservation 24: 38-49
# Liu, C., M. White & G. Newell, 2011. Measuring and comparing the accuracy of species distribution models with presence-absence data. Ecography 34: 232-243.
MLEvalLength <- 31
# Improve descriptions####
MLEval <- data.frame(Statistic = rep(NA, MLEvalLength),
Description = rep(NA, MLEvalLength),
Value = rep(NA, MLEvalLength))
MLEval[1,] <- c("Presence",
"n of presence data used",
e@np)
MLEval[2,] <- c("Absence",
"n of absence data used",
e@na)
MLEval[3,] <- c("AUC",
"Area under the receiver operator (ROC) curve",
e@auc)
if (length(e@pauc) == 0) e@pauc <- 0 # pauc may be missing, numeric(0), if so replace with 0
MLEval[4,] <- c("pAUC",
"p-value for the AUC (for the Wilcoxon test W statistic)",
e@pauc)
MLEval[5,] <- c("Cor",
"Correlation coefficient",
e@cor[[1]])
MLEval[6,] <- c("cor",
"p-value for correlation coefficient",
e@pcor)
# Steph Brodie's TSS which produces the same result as Allouche
# -1 just makes the output range is 0:1 instead of 1:2 I think.
# If so this means Sensitivity is e@TPR[which.max(e@TPR + e@TNR)], which doesn't include
# (e@TPR + e@FNR) but it's a vector of 1s so is redundant. Same for Specificity
MLEval[7,] <- c("TSS",
"True Skill Statistic",
max(e@TPR + e@TNR - 1))
# sensitivity: TP/(TP+FN)
MLEval[8,] <- c("Sensitivity",
"Sensitivity",
e@TPR[which.max(e@TPR + e@TNR)])
# specificity: TN/(FP+TN)
Specificity <- e@TNR[which.max(e@TPR + e@TNR)]
MLEval[9,] <- c("Specificity",
"Specificity",
Specificity)
# Accuracy: TP+TN / TP+TN+FP+FN true false positive negative.
# TP+TN is just TSS + 1, TP+TN+FP+FN #Sums to 2, redundant
MLEval[10,] <- c("Accuracy",
"Accuracy",
(e@TPR[which.max(e@TPR + e@TNR)] + e@TNR[which.max(e@TPR + e@TNR)]) / 2)
# Precision: TP/TP+FP. Ignores true negatives. “X% of the predictions are right”
Precision <- e@TPR[which.max(e@TPR + e@TNR)] / (e@TPR[which.max(e@TPR + e@TNR)] + e@FPR[which.max(e@TPR + e@TNR)])
MLEval[11,] <- c("Precision",
"X% of the predictions are right",
Precision)
# Recall: TP/TP+FN: “Y% of actually existing things are captured”.
Recall <- e@TPR[which.max(e@TPR + e@TNR)] / (e@TPR[which.max(e@TPR + e@TNR)] + e@FNR[which.max(e@TPR + e@TNR)])
MLEval[12,] <- c("Recall",
"Y% of actually existing things are captured",
Recall)
# https://www.corvil.com/kb/what-is-a-false-positive-rate
# Allouche et al 2006:
# overall accuracy: (TP+TN)/n
# this seems like a weird metric since the numerator is 0:2 or 1:2 and the divisor could be tiny or huge
MLEval[13,] <- c("OverallAccuracy",
"Overall Accuracy",
(e@TPR[which.max(e@TPR + e@TNR)] + e@TNR[which.max(e@TPR + e@TNR)])/nrow(samples))
# Balanced Accuracy, (Recall + Specificity) / 2
MLEval[14,] <- c("BalancedAccuracy",
"Balanced Accuracy",
(Recall + Specificity) / 2)
# Number of samples. Useful to include in the list
MLEval[15,] <- c("nSamples",
"Number of samples",
nrow(samples))
# Balance: precision vs recall curve. Workhorses.
# PxR/P+R = F score (P+R = harmonic mean).
# F1 score: P & R are equally rated. This is the most common one. F1 score importance depends on the project.
MLEval[16,] <- c("F1score",
"P & R equally rated, score importance depends on project",
2 * ((Precision * Recall) / (Precision + Recall)))
# F2 score: weighted average of Precision & Recall
MLEval[17,] <- c("F2score",
"weighted average of P & R",
5 * ((Precision * Recall) / (4 * Precision + Recall)))
# Threshold which produces the best combo of TPR & TNR
# t: vector of thresholds used to compute confusion matrices
MLEval[18,] <- c("Threshold",
"Threshold which produced best combo of TPR & TNR",
e@t[which.max(e@TPR + e@TNR)])
# e@prevalence: Prevalence
MLEval[19,] <- c("Prevalence",
"Prevalence",
e@prevalence[which.max(e@TPR + e@TNR)])
# e@ODP: Overall diagnostic power
MLEval[20,] <- c("ODP",
"Overall diagnostic power",
e@ODP[which.max(e@TPR + e@TNR)])
# e@CCR: Correct classification rate
MLEval[21,] <- c("CCR",
"Correct classification rate",
e@CCR[which.max(e@TPR + e@TNR)])
# e@TPR: True positive rate
MLEval[22,] <- c("TPR",
"True positive rate",
e@TPR[which.max(e@TPR + e@TNR)])
# e@TNR: True negative rate
MLEval[23,] <- c("TNR",
"True negative rate",
e@TNR[which.max(e@TPR + e@TNR)])
# e@FPR: False positive rate
MLEval[24,] <- c("FPR",
"False positive rate",
e@FPR[which.max(e@TPR + e@TNR)])
# e@FNR: False negative rate
MLEval[25,] <- c("FNR",
"False negative rate",
e@FNR[which.max(e@TPR + e@TNR)])
# e@PPP: Positive predictive power
MLEval[26,] <- c("PPP",
"Positive predictive power",
e@PPP[which.max(e@TPR + e@TNR)])
# e@NPP: Negative predictive power
MLEval[27,] <- c("NPP",
"Negative predictive power",
e@NPP[which.max(e@TPR + e@TNR)])
# e@MCR: Misclassification rate
MLEval[28,] <- c("MCR",
"Misclassification rate",
e@MCR[which.max(e@TPR + e@TNR)])
# e@OR: Odds-ratio
MLEval[29,] <- c("OR",
"Odds-ratio",
e@OR[which.max(e@TPR + e@TNR)])
# e@kappa: Cohen's kappa
MLEval[30,] <- c("kappa",
"Cohen's kappa",
e@kappa[which.max(e@TPR + e@TNR)])
# dev from calc.deviance from dismo
MLEval[31,] <- c("dev",
"deviance from 2 vecs, obs & pred vals",
dev)
View(MLEval)
plot(e, 'ROC')
plot(e, 'kappa')
plot(e, 'FPR')
plot(e, 'prevalence')
View(e)
library(dismo)
plot(e, 'ROC')
plot(e, 'kappa')
plot(e, 'FPR')
plot(e, 'prevalence')
getwd()
png(filename = paste0("./",names(samples[i]),"/Eval_ROC.png"),
width = 4*480, height = 4*480, units = "px", pointsize = 4*12,
bg = "white", res = NA, family = "", type = pngtype)
paste0("./",names(samples[i]),"/Eval_ROC.png")
setwd("/home/simon/Documents/Si Work/PostDoc Work/Kroetz & Dedman Sawfish BRT/")
png(filename = paste0("./",names(samples[i]),"/Eval_ROC.png"),
width = 4*480, height = 4*480, units = "px", pointsize = 4*12,
bg = "white", res = NA, family = "", type = pngtype)
par(mar = c(2.5,0.3,0,0.5), fig = c(0,1,0,1), cex.lab = 0.5, mgp = c(1.5,0.5,0), cex = 1.3, lwd = 6)
plot(e, 'ROC')
dev.off()
png(filename = paste0("./",names(samples[i]),"/Eval_ROC.png"))
plot(e, 'ROC')
dev.off()
png(filename = paste0("./",names(samples[i]),"/Eval_kappa.png"))
plot(e, 'kappa')
dev.off()
png(filename = paste0("./",names(samples[i]),"/Eval_FPR.png"))
plot(e, 'FPR')
dev.off()
png(filename = paste0("./",names(samples[i]),"/Eval_prevalence.png"))
plot(e, 'prevalence')
dev.off()
plot(e, 't')
plot(e, 'ODP')
plot(e, 'CCR')
plot(e, 'TPR')
plot(e, 'TNR')
plot(e, 'FNR')
plot(e, 'PPP')
plot(e, 'NPP')
plot(e, 'MCR')
plot(e, 'OR')
evalmetrics <- c("ROC", "kappa", "prevalence", "TPR", "TNR", "FPR", "FNR", "CCR", "PPP", "NPP", "MCR", "OR")
for (s in evalmetrics) {
png(filename = paste0("./",names(samples[i]),"/Eval_", s, ".png"))
plot(e, s)
dev.off()
}
